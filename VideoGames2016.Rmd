---
title: "Data Science: Captone (HarvardX: PH125.9x) - CYP Report"
author: "Ivory Wu"
date: "17 June 2020"
output: pdf_document
---

```{r setup, echo=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)

```

# Executive Summary

This is a report of Choose Your Own project for Data Science: Captone (HarvardX: PH125.9x). The author chose a dataset of Video Games Sales 2016 from Kaggle for the project. 

The objective of the project is to find the most suitable model to predict the user review score of individual games from the dataset. It could be used to improve the developers' and publishers' understanding of gamers' preference for future games. 

## Dataset 

The project uses the Video Games Sales dataset from Kaggle (https://www.kaggle.com/gregorut/videogamesales). This dataset is consist of video games with it total sales greater than 100,000 copies as of year 2016. 
  

```{r packages, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
# Load packges if required
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(glmnet)) install.packages("glmnet", repos = "http://cran.us.r-project.org")

# Download csv file from Github
mydata <- read.csv("https://raw.githubusercontent.com/IvyYW/Video_Games_2016/master/Video_Games_2016.csv",header=TRUE)

# Drop NA, unused data and convert year to numeric
mydata_clean <- mydata %>% drop_na() %>% 
    mutate(Year_of_Release = 1979 + as.numeric(Year_of_Release), User_Score = as.numeric(as.character(User_Score))) %>%
    select(-NA_Sales, -EU_Sales, -JP_Sales, -Other_Sales, -Publisher, -Developer) %>% 
    filter(Year_of_Release %in% c(1996:2016)) 
# Create Validation Set
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = mydata_clean$User_Score, times = 1, p = 0.1, list = FALSE)
VideoGames <- mydata_clean[-test_index,]
validation <- mydata_clean[test_index,]
```

Structure of the dataset:   
  
```{r}
str(VideoGames)
```

The dataset has 6199 observations with 10 variables, including 4 catagorical variables (Name, Platfrom, Genre & Rating) and 6 numeric variables (Year_of_Release, Global_Sales, Critic_Score, Critic_Count, User_Score & User_Count). 
\newpage  

## Variables

```{r}
summary(VideoGames)
```

**Catagorical Variables:**  

* Name: The title of individual games.   

* Platform: The platform that a video game could be executed on, and can be catagorised into 4 major groups by manufactures:
    1. Play Station (PS, PS2, PS3, PS4, PSP, PSV)
    2. XBOX (XB, X360, XOne)
    3. Nitendo (GB, GBA, Wii,WiiU, DS, 3DS, DC)
    4. Microsoft (PC)   
PS2 was the post popular platform in terms of numbers of games released on the platfrom.   

* Rating: Video game content rating system. Entertainment Software Rating Board (ESRB) is used in this dataset. 
    1. E rated games are suitable for all ages.
    2. E10+ for age 10 and above.
    3. T for age 13 and above.
    4. M for age 17 and above.
    5. AO for age 18 and above.  
T was the most used rating followed by E.  

* Genres: A specific category of games related by similar gameplay characteristics. In this dataset there are 13 genres accounted for: Action, Advebture, Fighting, Misc, Platfrom, Puzzle, Racing, Role-Playing, Shooter, Simulation, Sports and Startegy.   
Action is the most popular Genre by a margin (1457) to Sports in the second place (867).   

\newpage  
  
**Numeric Variables:** 

* Year of Release: The year that a game was released,ranging from 1996 to 2016.
* Global Sales: Copies of a game sold worldwide by millions. Only the games sold more than 100,000 copies were recorded in the dataset.
* Critic Score: Average review scores from recognised critics in the gaming industry. The score is between 0 and 100 points and the mean is 70.34.
* Critic Count: Number of reviews given by critics. In this dataset it ranges from 3 to 113 with a mean of 28.89.
* User Score: Average review score submitted by users. It is a 10-point system with a mean of 7.185.
* User Count: Number of reviews given by users. In this dataset it ranges from 4 to 10179 with a median of 27.

## Objective 

The objective of the project is to find the most suitable model to predict **User Score** from other variables in the dataset. The Root Mean Square Error (RMSE) bewteen the precited value and the actual result will be used to measure the effectiveness of the model. A random validation set that consist of 10% of the original dataset and is not used in the modeling will be used to verify the selected model.   
  
```{r}
RMSE <- function(true_rating, pred_rating){
  sqrt(mean((true_rating - pred_rating)^2))}

```

## Key Steps

1. **Data Screening and Cleaning**: Going through the data for the first time to remove unused or nominal data, observations that contains N/A values and nomalize certain variables for further caculation needed in the later stage. Validation set is created at the end of this stage to avoid interfering with the process. 

2. **Data Exploration and Analysis**: Look at the structure and the summary of the dataset, including the specifity/class of each variables and the observations it contents. Fitting of linear model on all available variables is used to determine the coeficient and to prioritize further data visualization.

3. **Data visualization**: 
    + Catagorical Variables: Create boxplots of user scores by levels within each variables to visualise if there are significant variance between different levels. 
    + Numeric Variables: Create Pointplots or lineplot to see if there is a trend between user scores and the variable. 
    
4. **Create Train Set and Test Set**: Create a random test set that consist 10% of the dataset (after the validation set is created) to be used to determine the effectiveness of each model trained by the train set. 
```{r echo=FALSE, warning=FALSE}
#Create train and test sets
set.seed(66, sample.kind="Rounding")

test_index <- createDataPartition(y = VideoGames$User_Score, times = 1, p = 0.1, list = FALSE)
train_set<- VideoGames[-test_index,]
test_set <- VideoGames[test_index,]

```
5. **Build Modeling**: 4 type of models were explored to find the most suitable with lowest RMSE:
    1. Generic Effect: Generic effect of Platform, Rating and Genre
    2. Linear model
    3. Random Forest
    4. Regression: Ridge & Lasso  

6. Compute RMSE with Validation Set: Verify the selected model with validation set

\newpage

# Method

## Data Cleaning
```{r eval=FALSE, include= TRUE}
# Drop NA, unused data and convert year to numeric
mydata_clean <- mydata %>% drop_na() %>% 
    mutate(Year_of_Release = 1979 + as.numeric(Year_of_Release), 
           User_Score = as.numeric(as.character(User_Score))) %>%
    select(-NA_Sales, -EU_Sales, -JP_Sales, -Other_Sales, -Publisher, -Developer) %>% 
    filter(Year_of_Release %in% c(1996:2016)) 
```

* Remove observations that includes any NA value.
* Year_of_Release variable was originally catagorical and was transformed into continuous numeric variable for data visualization purposes. 
* Remove the variables that are will not be used, nominal or consume too much memories when modeling (author is using a Macbook Air and has limitations).
* Filter Year_of_Release to comtemporary video games published after 1996. 

## Data Exploration & Visualisation  

First we look at the **distribution** of User_Score:  
  
  
```{r echo=FALSE, fig.width = 5, fig.align = "center"}
hist(train_set$User_Score)
summary(train_set$User_Score)
```
  
  
We can see from the summary and the histogram that the distribution is bell-shaped and has a flatter left shoulder to the sharp right shoulder. The mean (7.183) and median (7.5) are both within the 0.5 point range from the scale with most User Scores (7.5-8.0). 

\newpage

Then we fit a linear model to understand the relationship between User_Score and other variables in the dataset:   
    
```{r echo=FALSE}
#Look at coeficient & p-value of variables 
train_fit <- train_set %>% select(-Name) 
fit <- lm (User_Score~. , train_fit)
summary(fit)
```
   
    
While from the R-squared value (45%) it does show certain level of relevance between User_Score and other variables, it is too low to indicate a well-fitted model. However, if we divide the variables into two sets base on their class, we can see that the numeric variables all have p-values lower than 0.005, and the majority of the catagorical variables have p-values higher than 0.005. It shows that the numeric variables pass the significance test. 

In the next section, we take a deeper look at the catagorical variables: 

### Rating vs. User Score  

Apart from the three Rating that only contains one observation, all other Raings has similar median and box size plus slightly more variance in whiskers.  
  
    
```{r echo=FALSE, fig.width=5, fig.align = "center"}
Rating_Box <- train_set %>% ggplot(aes(x=User_Score, y = Rating)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 1) + 
  stat_boxplot(geom = 'errorbar') 
Rating_Box
```

\newpage

### Genre vs. User Score  

We see a similar boxplot as above. All genres have its User Score median very close to the oeverall median of 7.5 and similar box size. It implies that the variance of User Score between different levels within Rating/Genre variables are not significant, though the difference still exist and might be useful for applying individual effect in formulas.   
  
    
```{r echo=FALSE, fig.width = 5, fig.align = "center"}
Genre_Box <- train_set %>% 
  ggplot(aes(x = Genre, y = User_Score)) +
  geom_boxplot(aes(color = Genre), show.legend = FALSE) + 
  theme(axis.text.x = element_text(size=10, angle=45))
Genre_Box
```
  
\newpage
  
### Platform vs. User Score  
  
On the other hand, we see more variance of User Score between difference platfroms in the boxplot below, both in terms of the median and the distance between 1st and 3rd quartiles. One possible explanation is the fact that while a user would make purchases of video games across multiple genres and ratings, the platform they can play the game is restricted to the game consoles that a user owns. Thus the platform effect refects a fixed user base that creates a user effect on the User Score.   
  
```{r echo=FALSE, fig.width = 5, fig.align = "center"}
Platform_Box <- train_set %>% ggplot(aes(x=User_Score, y = Platform)) + 
  geom_boxplot(outlier.colour = "red", outlier.shape = 1) + 
  stat_boxplot(geom = 'errorbar') 
Platform_Box
```
  
\newpage

### Year of Release vs Average User Score   
  
We can see a trend of decreasing User Score from 1996 to 2010, then it bounced between the same range between the same range since then. It is consistant with the negative coeficient (-6.421e-02) observed in the linear model fitting and that Year of Realease and User Score has  negative correlation.   
  
```{r echo=FALSE, fig.width = 5, fig.align = "center"}
Year_Line <- train_set %>% group_by(Year_of_Release) %>% 
  summarise(avg_score = mean(User_Score)) %>%
  ggplot(aes(x=Year_of_Release, y = avg_score)) + 
  geom_line() 
Year_Line
```

\newpage
  
### User Count vs User Score  
  
Meanwhile, User Count only has positive correlations with User Score when the User Count is very small (< 250) or very big ( > 7500 ). Between the two marks there are almost no coorelations between the two variables, which is consistant with its small coefficient (3.694e-03).   
  
```{r echo=FALSE, message=FALSE, fig.width = 5, fig.align = "center"}
User_Count_point <- train_set %>% ggplot(aes(x = User_Count, y = User_Score)) +
  geom_point() +
  geom_smooth(method = "loess", span = 0.3)
User_Count_point
```

\newpage

### Critic Score vs User Score  
  
Lastly, Critic Score has positive correlations with User Scores across all range. It does make sense that Critics and Users have similar reviews towards a specific video game and it is consistant with its postitive coefficcient (6.479e-02).   
  
```{r echo=FALSE, message=FALSE, fig.width = 5, fig.align = "center"}
Critic_Score_point <- train_set %>% ggplot(aes(x = Critic_Score, y = User_Score)) +
  geom_point() +
  geom_smooth(method = "loess", span = 0.3)
Critic_Score_point
```

\newpage

## Modeling Approach

We first approach the modeling with basic General Effect Model with 3 catagorical variables:  

$$\hat{Y} = mu + b_p + b_g + b_r$$  

$\hat{Y}$: Predicted value    
mu: Average User Score    
$b_p$: Platform Effect    
$b_g$: Genre Effect    
$b_r$: Rating Effect    
   
### Model 1: Average User Score

$$\hat{Y} = mu$$

We calculate the average User Score across train set as predicted vales.   
  
```{r}
# compute average User_Score
mu <- mean(train_set$User_Score)
# compute predicted value
pred_1 <- rep(mu, nrow(test_set))
# compute RMSE
rmse_1 <- RMSE(test_set$User_Score, pred_1)
# add value to results table
rmse_results <- tibble(method = "Model 1: Average Score", RMSE = rmse_1)
rmse_results
```

\newpage

### Model 2: Platform Effect

$$\hat{Y} = mu + b_p$$

We calculate Platfrom effect ($b_p$) for each Platfrom from the train set by obtaining the average difference from the mean after grouping the dataset by Platfrom. Then we left join the value to test set by Platform to compute the predicted value for test set. 

```{r}
#compute b_p: platfrom effect
Platform_effect <- train_set %>% group_by(Platform) %>% 
  summarise(b_p = mean(User_Score) - mu) 
train_effect <- train_set %>% left_join(Platform_effect, by = "Platform")

# compute predicted value
pred_2 <- test_set %>% mutate(mu = mu) %>% 
  left_join(Platform_effect, by = "Platform") %>%
  mutate(pred = mu + b_p) %>% pull(pred)

#comput RMSE and update result table
rmse_2 <- RMSE(test_set$User_Score, pred_2)
rmse_results <- bind_rows(rmse_results, 
                          tibble(method="Model 2: Platform Effect", 
                                 RMSE = rmse_2))
rmse_results %>% kable()
```

\newpage

### Model 3: Platform + Genre Effect

$$\hat{Y} = mu + b_p + b_g$$

Again we calculate the Genre effect ($b_g$) the same way as Model 2 but grouping the train set by Genre instead. Left join the value to test set by Genre to compute the predicted vale. 

```{r}
#compute b_g: genre effect
Genre_effect <- train_effect %>% group_by(Genre) %>% 
  summarise(b_g = mean(User_Score) - mu - mean(b_p))
train_effect <- train_effect %>% left_join(Genre_effect, by = "Genre")

# compute predicted value
pred_3 <- test_set %>% mutate(mu = mu) %>% 
  left_join(Platform_effect, by = "Platform") %>%
  left_join(Genre_effect, by = "Genre") %>%
  mutate(pred = mu + b_p +b_g ) %>% pull(pred)

#comput RMSE and update result table
rmse_3 <- RMSE(test_set$User_Score, pred_3)
rmse_results <- bind_rows(rmse_results, 
                          tibble(method="Model 3: Platfrom + Genre Effect", 
                                 RMSE = rmse_3))
rmse_results %>% kable()
```

\newpage

### Model 4: Platform + Genre + Rating Effect

$$\hat{Y} = mu + b_p + b_g + b_r$$

Repeat the same process for Rating Effect as the two models above.   

```{r}
#compute b_r: rating effect
Rating_effect <- train_effect %>% group_by(Rating) %>% 
  summarise(b_r = mean(User_Score) - mu - mean(b_p) - mean(b_g))
train_effect <- train_effect %>% left_join(Rating_effect, by = "Rating")

# compute predicted value
pred_3 <- test_set %>% mutate(mu = mu) %>% 
  left_join(Platform_effect, by = "Platform") %>%
  left_join(Genre_effect, by = "Genre") %>%
  left_join(Rating_effect, by = "Rating") %>%
  mutate(pred = mu + b_p +b_g + b_r ) %>% pull(pred)

#comput RMSE and update result table
rmse_4 <- RMSE(test_set$User_Score, pred_3)
rmse_results <- bind_rows(rmse_results, 
                          tibble(method="Model 4: Platform + Genre + Rating Effect", 
                                 RMSE = rmse_4))
rmse_results %>% kable()
```

We can see that by adding each variables to the formula, RMSE decreases slightly each time but remains an unsatisfying result above 1.3. In order to include more variables to improve the RMSE and based on the earlier analysis that all numeric variable have p-values below 0.05, we will fit a linear model with all numeric variable first.  

\newpage

### Model 5: Linear Model (Numeric Variables) 
  
```{r tidy=TRUE}
# fit linear model for numeric values
fit <- lm(User_Score ~ Year_of_Release + Global_Sales + User_Count + 
            Critic_Count + Critic_Score, train_set)

# compute predicted value
pred_5 <- predict(fit, test_set)

#comput RMSE and update result table
rmse_5 <- RMSE(test_set$User_Score, pred_5)
rmse_results <- bind_rows(rmse_results, 
                          tibble(method="Model 5: Linear Model (Numeric Variables)", 
                                 RMSE = rmse_5))
rmse_results %>% kable()
```

The RMSE from Model 5 is significantly lower that Model 1 to 4. In the earlier anaysis it also showed that some levels within each catagorical variables also have p-vaule lower than 0.005. Therefor we will try to fit the Linear Model again with all variables:   

\newpage

### Model 6: Linear Model (All Variables)
```{r tidy=TRUE}
# fit linear model for all values
fit <- lm(User_Score ~ Platform + Genre + Rating + Global_Sales + 
            User_Count + Critic_Count + Critic_Score, train_set)

# compute predicted value
pred_6 <- predict(fit, test_set)

#comput RMSE and update result table
rmse_6 <- RMSE(test_set$User_Score, pred_6)
rmse_results <- bind_rows(rmse_results, 
                          tibble(method="Model 6: Linear Model (All Variables)", 
                                 RMSE = rmse_6))
rmse_results %>% kable()
```

The RMSE result is slightly lower than Model 5. It implies the catagorical variables are worth including to improve the result though the impact is not significant. 

\newpage

Next, we move on to Random Forest model in pursue of higher accuracy. It should be a more suitable choice for the dataset as it consist multi-dimentional catagorical and continuous variables.   

### Model 7: Random Forest 

```{r}
train_rf <- train_set %>% select(-Name) 

# fit random forest model with minimum OOB
fit_rf <- randomForest(User_Score~., train_rf, ntree = 100)

# compute predicted value
pred_7 <- predict(fit_rf, test_set)

#comput RMSE and update result table
rmse_7 <- RMSE(test_set$User_Score, pred_7)
rmse_results <- bind_rows(rmse_results, 
                          tibble(method="Model 7: Random Forest", 
                                 RMSE = rmse_7))
rmse_results %>% kable()
```

Random Forest had indeed reduced RMSE significantly compared to the results from linear models. The plot below shows that we reach the minumum OOB at 100 for ntree and therefor any further increase in ntree value will not improve the result. 
  

```{r}
#Plot minimum OOB
plot(fit_rf)
```

We can also see the variable importance in the Random Forest below. It does echo what we have discovered in the data visualition section that the most obvious trend was between Critic Score and User Score, and that Platfrom has the most variace between levels among all catagorical variables.   
   
```{r}
# variable importance 
importance(fit_rf)
#Plot variable importance
varImpPlot(fit_rf)
```

\newpage  
  
Lastly we look at the Regularization models. We use Ridge and Lasso Regression as the dataset has certain level of multicollinearity (ex. User Count and Crtic Count).   
  
### Model 8: Ridge Regression 

```{r}
#Prep matrix for the model
x_var <- data.matrix(train_set[, -c(1,8)])
y_var <- train_set$User_Score
lambda_seq <- 10^seq(-2, 5, length = 100)

# fit the model for minimum lambda
fit_cv <- cv.glmnet(x_var, y_var, alpha = 0, lambda = lambda_seq)
lambda_min <- fit_cv$lambda.min

# fit ridge regression model with min lambda
fit_ridge <- glmnet(x_var, y_var, alpha = 0, lambda  = lambda_min)

# prep test set - remove name and User_score
test_x <- data.matrix(test_set[, -c(1,8)])
# compute predicted values
pred_8 <- predict(fit_ridge, s = lambda_min, newx = test_x)
# compute RMSE and update results table
rmse_8 <- RMSE(test_set$User_Score, pred_8)
rmse_results <- bind_rows(rmse_results, 
                          tibble(method="Model 8: Ridge Regression ", 
                                 RMSE = rmse_8))
rmse_results %>% kable()
```

\newpage

### Model 9: Lasso Regression 

```{r}
# fit the model for minimum lambda
fit_la <- cv.glmnet(x_var, y_var, alpha = 1)
lambda_min <- fit_la$lambda.min

# fit lasso regression model with min lambda
fit_lasso <- glmnet(x_var, y_var, alpha = 1, lambda  = lambda_min)
# prep test set - remove name and User_score
test_x <- data.matrix(test_set[, -c(1,8)])
# compute predicted values
pred_9 <- predict(fit_lasso, s = lambda_min, newx = test_x)
# compute RMSE and update results table
rmse_9 <- RMSE(test_set$User_Score, pred_9)
rmse_results <- bind_rows(rmse_results, tibble(method="Model 9: Lasso Regression ", RMSE = rmse_9))
rmse_results %>% kable()
```

\newpage

# Results

```{r echo=FALSE}
rmse_results %>% kable()
```

We can see from the table that **Random Forest** model produces the lowest RMSE, followed by **Linear Model** then **Regularization models** in a close 3rd place. 
Random Forest has been the most successful model because:  
    1. Its flexibility to accomodate multidimentional variables.
    2. The dataset has both catagorical and continuous numeric values.
    3. In the categorical variables, only certain levels has significant impact on the prediction and Random Forest can capture this trait.   

We now verify the validation set with Random Forest: 
```{r}
# compute predicted value for validation set
pred_vali <- predict(fit_rf, validation)

#comput RMSE and update result table
rmse_vali <- RMSE(validation$User_Score, pred_vali)
rmse_vali
```
The RMSE for the validation set is `r rmse_vali`.

\newpage

# Conclusion 

## Summary

The report focused on predicting the User Score from the variables avaiable from the Video Games 2016 dataset, including Catagorical variables (Platform, Rating and Genre) and numeric variables (Year of Release, Global Sales, User Count, Critic Count and Critic Score). 

In data exploration and analysis we foused on the visualisation of User Score among the levels for catagorical variables, as well as the significance of each variables through linear model diagnosis. 

The dataset was then divided into train set and test set to build modeling to predict values that produces the lowest RMSE. Among the Generic Effect (Platform, Rating and Genre), Linear Model, Random Forest and Regularizations (Ridge and Lasso Regression) models, we found that Random Forest produces the lowest RMSE thus the most suitable model to predict User Score. 

## Potential Impact

The algorithm will be able to help the Gaming industry predict what review scores a video games is likely to receive from the users, and use the machine learning concept to make better decisions based on the predicted user score from this algorithm.

## Limitations 

The author runs the project on an Macbook Air 2014 with OSX El Capitan. Due to the limitation on memory, some variables (Regional Sales and Publishers/Developers) had to be removed at the Data Cleaning stage to accomodate the fitting of Linear Model , Random Forest and Regularization. 

It is very likely that the RMSE would be lower for all the models used in the project if the author can accomodation more variables, as Regional Sales will provide insights that reflects the preference of the users from different regions, as well as the correlation with Platforms and Genre. The Publisher/Developer variables will show us if brand loyalty affect User Scores.

## Future Work

For the gaming industry, they could further explore the optimized combination for the games that record higher user scores - especially for the parameters that could be pre-determined at the planning stage of game developement. For example, Sports games for PS4 receive higher user scores due to the comfort of the controller, or Thriller games tend to receive higher user scores when it is rated for older age groups. 



